{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3b104e",
   "metadata": {},
   "source": [
    "# GE461 Project 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d679236",
   "metadata": {},
   "source": [
    "#### Install dependicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b0268e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from skmultiflow.trees import HoeffdingTreeClassifier\n",
    "from skmultiflow.data import SEAGenerator, AGRAWALGenerator, ConceptDriftStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d0fb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Helper: wrap any two generators into an abrupt ConceptDriftStream ----\n",
    "def make_drift(base_gen, drift_gen, position, width=1, seed=None):\n",
    "    \"\"\"Return a ConceptDriftStream with a single abrupt drift.\"\"\"\n",
    "    return ConceptDriftStream(\n",
    "        stream=base_gen,\n",
    "        drift_stream=drift_gen,\n",
    "        position=position,\n",
    "        width=width,\n",
    "        random_state=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eca70bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_drifts(gen0, gen1, gen2, pos1, pos2, width=1):\n",
    "    \"\"\"Nest two ConceptDriftStreams so that gen0→gen1 at pos1, gen1→gen2 at pos2.\"\"\"\n",
    "    first_drift  = make_drift(gen0, gen1, position=pos1, width=width)\n",
    "    second_drift = make_drift(first_drift, gen2, position=pos2, width=width)\n",
    "    second_drift.prepare_for_use()          # initialise internal RNGs\n",
    "    return second_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d1a7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dfcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mustafacankanbalci/miniconda3/envs/datascience5/lib/python3.9/site-packages/skmultiflow/data/base_stream.py:191: FutureWarning: 'prepare_for_use' has been deprecated in v0.5.0 and will be removed in v0.7.0.\n",
      "New instances of the Stream class are now ready to use after instantiation.\n",
      "  warnings.warn(\n",
      "/Users/mustafacankanbalci/miniconda3/envs/datascience5/lib/python3.9/site-packages/skmultiflow/data/concept_drift_stream.py:160: RuntimeWarning: overflow encountered in exp\n",
      "  probability_drift = 1.0 / (1.0 + np.exp(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved stream to SEADataset.csv.gz\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from skmultiflow.data import SEAGenerator, ConceptDriftStream\n",
    "from skmultiflow.meta import AdaptiveRandomForestClassifier\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 1. Build a SEA stream with drifts at 35 k and 60 k (width = 1)\n",
    "POS1, POS2, WIDTH, N_SAMPLES = 35_000, 60_000, 1, 100_000\n",
    "\n",
    "sea0 = SEAGenerator(classification_function=0, random_state=1)\n",
    "sea1 = SEAGenerator(classification_function=1, random_state=2)\n",
    "sea2 = SEAGenerator(classification_function=2, random_state=3)\n",
    "\n",
    "drift1 = ConceptDriftStream(stream=sea0, drift_stream=sea1,\n",
    "                            position=POS1, width=WIDTH, random_state=7)\n",
    "sea_stream = ConceptDriftStream(stream=drift1, drift_stream=sea2,\n",
    "                                position=POS2, width=WIDTH, random_state=8)\n",
    "sea_stream.prepare_for_use()\n",
    "\n",
    "# (optional) persist once for reproducibility\n",
    "save_path = Path(\"SEADataset.csv\")\n",
    "if not save_path.exists():\n",
    "    import csv, gzip\n",
    "    with gzip.open(save_path.with_suffix(\".csv.gz\"), \"wt\", newline=\"\") as gz:\n",
    "        wr = csv.writer(gz)\n",
    "        wr.writerow([f\"x{i}\" for i in range(sea_stream.n_features)] + [\"y\"])\n",
    "        for _ in range(N_SAMPLES):\n",
    "            X, y = sea_stream.next_sample()\n",
    "            wr.writerow(np.append(X[0], y[0]))\n",
    "    print(f\"✓ Saved stream to {save_path}.gz\")\n",
    "    sea_stream.restart()               # rewind for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41da52c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:09<00:00,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adaptive Random Forest (restored via pandas)\n",
      "100,000 samples analysed\n",
      "Accuracy: 0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from skmultiflow.meta import AdaptiveRandomForestClassifier\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 1. Load the saved stream with pandas\n",
    "csv_file = \"SEADataset.csv.gz\"          # \".gz\" extension is optional in read_csv\n",
    "chunk_sz = 100                       # read in manageable pieces\n",
    "\n",
    "reader = pd.read_csv(\n",
    "    csv_file,\n",
    "    compression=\"infer\",                # auto‑detect .gz\n",
    "    chunksize=chunk_sz                  # returns an iterator of DataFrame chunks\n",
    ")\n",
    "\n",
    "# 2. Re‑initialise the classifier (fresh run)\n",
    "arf = AdaptiveRandomForestClassifier()\n",
    "\n",
    "n_samples   = 0\n",
    "correct_cnt = 0\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 3. Iterate chunk‑by‑chunk, row‑by‑row (simulating an online stream)\n",
    "for chunk in tqdm(reader, total=100_000 // chunk_sz):\n",
    "    # fast vector → NumPy for efficiency (but still loop row‑wise)\n",
    "    X_chunk = chunk.drop(columns=\"y\").values\n",
    "    y_chunk = chunk[\"y\"].values\n",
    "\n",
    "    for X_i, y_i in zip(X_chunk, y_chunk):\n",
    "        X_i = X_i.reshape(1, -1)        # scikit‑multiflow expects 2‑D array\n",
    "\n",
    "        # test phase\n",
    "        y_pred = arf.predict(X_i)\n",
    "        if y_pred.size and (y_pred[0] == y_i):\n",
    "            correct_cnt += 1\n",
    "\n",
    "        # train phase\n",
    "        arf.partial_fit(X_i, [y_i])\n",
    "\n",
    "        n_samples += 1\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "print(\"\\nAdaptive Random Forest (restored via pandas)\")\n",
    "print(f\"{n_samples:,d} samples analysed\")\n",
    "print(f\"Accuracy: {correct_cnt / n_samples:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0rows [00:00, ?rows/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['y'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m args \u001b[38;5;241m=\u001b[39m ap\u001b[38;5;241m.\u001b[39mparse_args()\n\u001b[1;32m    135\u001b[0m model \u001b[38;5;241m=\u001b[39m make_model(args\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 136\u001b[0m acc, n \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m – analysed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instances\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 112\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(file, model, chunksize)\u001b[0m\n\u001b[1;32m    108\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m tqdm(reader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 112\u001b[0m     X_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    113\u001b[0m     y_chunk \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m Xi, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_chunk, y_chunk):\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience5/lib/python3.9/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience5/lib/python3.9/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience5/lib/python3.9/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience5/lib/python3.9/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['y'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "from collections import deque\n",
    "from itertools import cycle\n",
    "from typing import List, Type\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skmultiflow.trees import HoeffdingTreeClassifier\n",
    "from skmultiflow.meta import AdaptiveRandomForestClassifier\n",
    "from skmultiflow.drift_detection import DDM, EDDM, ADWIN\n",
    "# If you need SAM‑kNN later:\n",
    "# from skmultiflow.lazy import SAMKNNClassifier\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 1 ·  Building‑block classes for the two custom ensembles\n",
    "class SlidingWindowAccuracy:\n",
    "    def __init__(self, size: int = 100):\n",
    "        self.win = deque(maxlen=size)\n",
    "    def update(self, correct: bool):\n",
    "        self.win.append(1 if correct else 0)\n",
    "    @property\n",
    "    def value(self) -> float:\n",
    "        return np.mean(self.win) if self.win else 0.0\n",
    "\n",
    "\n",
    "class ActiveLearner:\n",
    "    def __init__(self, detector_cls: Type[DDM], window: int = 100):\n",
    "        self.clf = HoeffdingTreeClassifier()\n",
    "        self.det = detector_cls()\n",
    "        self.acc = SlidingWindowAccuracy(window)\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict([X])[0]\n",
    "    def update(self, X, y):\n",
    "        y_hat   = self.predict(X)\n",
    "        correct = y_hat == y\n",
    "        self.acc.update(correct)\n",
    "        self.det.add_element(int(not correct))\n",
    "        self.clf.partial_fit([X], [y], classes=[0, 1])\n",
    "        return correct\n",
    "    def drift_detected(self):\n",
    "        return getattr(self.det, \"detected_change\", False)\n",
    "\n",
    "\n",
    "class _BaseEnsemble:\n",
    "    DETECTORS = [DDM, EDDM, ADWIN]\n",
    "    def __init__(self, n_learners=3, window=100, **kwargs):\n",
    "        detector_cycle = cycle(self.DETECTORS)\n",
    "        self.window   = window\n",
    "        self.learners: List[ActiveLearner] = [\n",
    "            ActiveLearner(next(detector_cycle), window) for _ in range(n_learners)\n",
    "        ]\n",
    "    def predict(self, X):\n",
    "        votes = {}\n",
    "        for lr in self.learners:\n",
    "            label = lr.predict(X)\n",
    "            w = lr.acc.value or 1e-6\n",
    "            votes[label] = votes.get(label, 0.0) + w\n",
    "        return max(votes.items(), key=lambda kv: (kv[1], -kv[0]))[0]\n",
    "    def update(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        for i, lr in enumerate(self.learners):\n",
    "            lr.update(X, y)\n",
    "            if self._should_reset(lr):\n",
    "                self.learners[i] = ActiveLearner(type(lr.det), self.window)\n",
    "        return y_hat\n",
    "    def _should_reset(self, lr: ActiveLearner):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ActiveDriftEnsemble(_BaseEnsemble):\n",
    "    def _should_reset(self, lr):\n",
    "        return lr.drift_detected()\n",
    "\n",
    "\n",
    "class PassiveDriftEnsemble(_BaseEnsemble):\n",
    "    def __init__(self, tau=0.70, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.tau = tau\n",
    "    def _should_reset(self, lr):\n",
    "        return lr.acc.value < self.tau\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 2 ·  Factory for the four evaluation models\n",
    "def make_model(name: str):\n",
    "    name = name.lower()\n",
    "    if name == \"arf\":\n",
    "        return AdaptiveRandomForestClassifier()\n",
    "    # elif name == \"samknn\":\n",
    "    #     return SAMKNNClassifier()\n",
    "    elif name == \"active\":\n",
    "        return ActiveDriftEnsemble()\n",
    "    elif name == \"passive\":\n",
    "        return PassiveDriftEnsemble()\n",
    "    else:\n",
    "        raise ValueError(\"model must be arf | samknn | active | passive\")\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 3 ·  Main evaluation routine\n",
    "def evaluate(file, model, chunksize=1000):\n",
    "    reader = pd.read_csv(file, compression=\"infer\", chunksize=chunksize)\n",
    "    n_samples = correct = 0\n",
    "\n",
    "    for chunk in tqdm(reader, total=None, unit=\"rows\"):\n",
    "\n",
    "        X_chunk = chunk.drop(columns=\"y\").values\n",
    "        y_chunk = chunk[\"y\"].values\n",
    "    \n",
    "\n",
    "        for Xi, yi in zip(X_chunk, y_chunk):\n",
    "            Xi = Xi.reshape(1, -1)\n",
    "            y_pred = model.predict(Xi)\n",
    "            if y_pred.size and (y_pred[0] == yi):\n",
    "                correct += 1\n",
    "            model.partial_fit(Xi, [yi]) if hasattr(model, \"partial_fit\") else model.update(Xi[0], yi)\n",
    "            n_samples += 1\n",
    "\n",
    "    return correct / n_samples, n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81671ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
